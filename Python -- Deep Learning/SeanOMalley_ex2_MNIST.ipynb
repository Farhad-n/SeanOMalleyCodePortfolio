{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST- handwritten digit recognition\n",
    "[The MNIST problem](http://yann.lecun.com/exdb/mnist/) is a dataset developed by Yann LeCun, Corinna Cortes and Christopher\n",
    "Burges for evaluating machine learning models on the handwritten digit classification problem.\n",
    "The dataset was constructed from a number of scanned document datasets available from the\n",
    "National Institute of Standards and Technology (NIST). This is where the name for the dataset\n",
    "comes from, as the Modified NIST or MNIST dataset.\n",
    "Images of digits were taken from a variety of scanned documents, normalized in size and\n",
    "centered. This makes it an excellent dataset for evaluating models, allowing the developer to\n",
    "focus on the machine learning with very little data cleaning or preparation required. Each\n",
    "image is a 28 Ã— 28 pixel square (784 pixels total).\n",
    "The dataset below has 60000 images for training and 10000 for testing. Labels are the 10 digits (0-9), therefore it is a multi-class classification problem. Excellent results achieve a prediction error of less than 1%. State-of-the-art prediction\n",
    "error of approximately 0.2% can be achieved with large Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap='gray') \n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap='gray')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
      "255 0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train.max(), X_train.min()) # Note that the pixel value scales 0-255 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using simple neural net\n",
    "MNIST problem is not terribly difficult that we may get a decent result by using simple neural nets without CNN. We can build a baseline model with a simple neural net and later compare how a CNN model does better that the baseline.\n",
    "\n",
    "A simple NN talkes 1D input, so we will change the data shape to a 1D vector: a 2D image of 28x28 pixels is reshaped (flattened) to a 1D vector of 784 pixels. Then the pixel values are normalized so that a value is between 0 and 1. Then we make labels (both y_train and y_test) to categorical- which means we make 10 columns with values 0 or 1 instead of 1 column with values 0-9.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import keras.utils as np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.1766 - acc: 0.1354 - val_loss: 2.1098 - val_acc: 0.1354\n",
      "Epoch 2/20\n",
      " - 1s - loss: 2.0554 - acc: 0.1668 - val_loss: 2.0134 - val_acc: 0.1717\n",
      "Epoch 3/20\n",
      " - 1s - loss: 1.9770 - acc: 0.2047 - val_loss: 1.9544 - val_acc: 0.2485\n",
      "Epoch 4/20\n",
      " - 1s - loss: 1.9257 - acc: 0.2390 - val_loss: 1.9122 - val_acc: 0.2373\n",
      "Epoch 5/20\n",
      " - 1s - loss: 1.8887 - acc: 0.2386 - val_loss: 1.8808 - val_acc: 0.2394\n",
      "Epoch 6/20\n",
      " - 1s - loss: 1.8604 - acc: 0.2448 - val_loss: 1.8565 - val_acc: 0.2542\n",
      "Epoch 7/20\n",
      " - 1s - loss: 1.8377 - acc: 0.2502 - val_loss: 1.8356 - val_acc: 0.2594\n",
      "Epoch 8/20\n",
      " - 1s - loss: 1.8187 - acc: 0.2511 - val_loss: 1.8183 - val_acc: 0.2664\n",
      "Epoch 9/20\n",
      " - 1s - loss: 1.8018 - acc: 0.2657 - val_loss: 1.8022 - val_acc: 0.2467\n",
      "Epoch 10/20\n",
      " - 1s - loss: 1.7862 - acc: 0.2639 - val_loss: 1.7874 - val_acc: 0.2977\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.7713 - acc: 0.2635 - val_loss: 1.7733 - val_acc: 0.2676\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.7578 - acc: 0.2773 - val_loss: 1.7607 - val_acc: 0.2990\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.7455 - acc: 0.2964 - val_loss: 1.7511 - val_acc: 0.3046\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.7347 - acc: 0.3118 - val_loss: 1.7407 - val_acc: 0.3299\n",
      "Epoch 15/20\n",
      " - 1s - loss: 1.7249 - acc: 0.3248 - val_loss: 1.7326 - val_acc: 0.3407\n",
      "Epoch 16/20\n",
      " - 1s - loss: 1.7163 - acc: 0.3349 - val_loss: 1.7252 - val_acc: 0.3405\n",
      "Epoch 17/20\n",
      " - 1s - loss: 1.7084 - acc: 0.3375 - val_loss: 1.7190 - val_acc: 0.3530\n",
      "Epoch 18/20\n",
      " - 1s - loss: 1.7015 - acc: 0.3439 - val_loss: 1.7136 - val_acc: 0.3493\n",
      "Epoch 19/20\n",
      " - 1s - loss: 1.6952 - acc: 0.3436 - val_loss: 1.7101 - val_acc: 0.3495\n",
      "Epoch 20/20\n",
      " - 1s - loss: 1.6894 - acc: 0.3478 - val_loss: 1.7023 - val_acc: 0.3511\n",
      "Baseline Error: 64.89%\n"
     ]
    }
   ],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,input_shape=(784,),activation='relu'))\n",
    "    model.add(Dense(10, activation = 'sigmoid'))\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN model\n",
    "Now we will build a simple CNN model and see if it can do better than our baseline model above (NN without convolution layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN for the MNIST Dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "import keras.utils as np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 123\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TURN\n",
    "# Reshape X_train and X_test to be (n_samples, width, height, channels)\n",
    "# Then cast the data type of the arrays to type 'float32'\n",
    "# Hint: the n_channel is 1 in this case. You can use .shape\n",
    "# Hint: .astype() changes numpy array's data type\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)).astype('float32')\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)).astype('float32')\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR TURN\n",
    "# Normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train /255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Encode labels to categorical\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_train)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "# create model\n",
    "### YOUR TURN\n",
    "# Build a model that has 1 convolution layer, 1 max pooling, 1 dense, and output \n",
    "# Use 32 filters with 5x5 size\n",
    "# For max pooling layer, make the layer such that the featuremap size would be the half after the pooling layer\n",
    "# hint: you need to change the argument input_shape to (w,h,1) in the first conv layer\n",
    "# hint: you need Flatten() before the first dense layer\n",
    "# Compile the model using the same options as above\n",
    "\n",
    "# define a simple CNN model\n",
    "def simple_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, kernel_size = (5,5), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Convolution2D(32, kernel_size = (5,5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = simple_cnn_model()\n",
    "\n",
    "print(model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 266s 4ms/step - loss: 0.3484 - acc: 0.9042\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0989 - acc: 0.9728\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 214s 4ms/step - loss: 0.0674 - acc: 0.9804\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 214s 4ms/step - loss: 0.0532 - acc: 0.9850\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0447 - acc: 0.9875\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 213s 4ms/step - loss: 0.0385 - acc: 0.9884\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0330 - acc: 0.9896\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0291 - acc: 0.9912\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0256 - acc: 0.9917\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0243 - acc: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138dc8fd0>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=200, verbose=1)\n",
    "#model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "#print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-layer CNN model\n",
    "Let's stack another convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN\n",
    "# Build a 2-conv layer model\n",
    "# Try this architecture: conv-maxpool-conv-maxpool-dense-dense-output\n",
    "# Use 32 (5x5) filters for the first conv layer, and use 16 (3x3) filters for the second conv layer\n",
    "# Use the same max pool as above\n",
    "# (optional) you can compare with and with out dropout layer after the second maxpool layer\n",
    "# Usng dropout is a way to regularize your model. Dropout randomly drops some neurons in the feature map.\n",
    "# By dropping out some units, it effectively makes the model smaller so it helps reducing the overfitting. \n",
    "# see https://keras.io/layers/core/#dropout for the use\n",
    "# Use 1-2 dense layers before the output\n",
    "\n",
    "def bilayer_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, kernel_size = (5,5), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Convolution2D(32, kernel_size = (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "   \n",
    "# build the model\n",
    "model = bilayer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 125s - loss: 0.4137 - acc: 0.8723\n",
      "Epoch 2/20\n",
      " - 124s - loss: 0.1147 - acc: 0.9708\n",
      "Epoch 3/20\n",
      " - 125s - loss: 0.0830 - acc: 0.9792\n",
      "Epoch 4/20\n",
      " - 124s - loss: 0.0622 - acc: 0.9842\n",
      "Epoch 5/20\n",
      " - 124s - loss: 0.0538 - acc: 0.9865\n",
      "Epoch 6/20\n",
      " - 124s - loss: 0.0464 - acc: 0.9879\n",
      "Epoch 7/20\n",
      " - 124s - loss: 0.0370 - acc: 0.9903\n",
      "Epoch 8/20\n",
      " - 124s - loss: 0.0335 - acc: 0.9915\n",
      "Epoch 9/20\n",
      " - 124s - loss: 0.0289 - acc: 0.9923\n",
      "Epoch 10/20\n",
      " - 124s - loss: 0.0227 - acc: 0.9934\n",
      "Epoch 11/20\n",
      " - 124s - loss: 0.0235 - acc: 0.9939\n",
      "Epoch 12/20\n",
      " - 125s - loss: 0.0200 - acc: 0.9942\n",
      "Epoch 13/20\n",
      " - 124s - loss: 0.0213 - acc: 0.9943\n",
      "Epoch 14/20\n",
      " - 124s - loss: 0.0192 - acc: 0.9945\n",
      "Epoch 15/20\n",
      " - 124s - loss: 0.0159 - acc: 0.9957\n",
      "Epoch 16/20\n",
      " - 124s - loss: 0.0128 - acc: 0.9963\n",
      "Epoch 17/20\n",
      " - 124s - loss: 0.0122 - acc: 0.9962\n",
      "Epoch 18/20\n",
      " - 124s - loss: 0.0154 - acc: 0.9955\n",
      "Epoch 19/20\n",
      " - 124s - loss: 0.0140 - acc: 0.9960\n",
      "Epoch 20/20\n",
      " - 124s - loss: 0.0103 - acc: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1380a4c88>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=200, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "#print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More convolution layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, kernel_size = (10,10), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Convolution2D(32, kernel_size = (5,5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.10))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = multilayer_cnn_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 121s - loss: 0.4082 - acc: 0.8723\n",
      "Epoch 2/20\n",
      " - 121s - loss: 0.1062 - acc: 0.9700\n",
      "Epoch 3/20\n",
      " - 120s - loss: 0.0749 - acc: 0.9792\n",
      "Epoch 4/20\n",
      " - 121s - loss: 0.0588 - acc: 0.9832\n",
      "Epoch 5/20\n",
      " - 120s - loss: 0.0499 - acc: 0.9860\n",
      "Epoch 6/20\n",
      " - 120s - loss: 0.0431 - acc: 0.9880\n",
      "Epoch 7/20\n",
      " - 120s - loss: 0.0366 - acc: 0.9903\n",
      "Epoch 8/20\n",
      " - 120s - loss: 0.0310 - acc: 0.9914\n",
      "Epoch 9/20\n",
      " - 120s - loss: 0.0272 - acc: 0.9923\n",
      "Epoch 10/20\n",
      " - 120s - loss: 0.0263 - acc: 0.9919\n",
      "Epoch 11/20\n",
      " - 120s - loss: 0.0231 - acc: 0.9934\n",
      "Epoch 12/20\n",
      " - 121s - loss: 0.0225 - acc: 0.9932\n",
      "Epoch 13/20\n",
      " - 121s - loss: 0.0178 - acc: 0.9947\n",
      "Epoch 14/20\n",
      " - 120s - loss: 0.0195 - acc: 0.9942\n",
      "Epoch 15/20\n",
      " - 120s - loss: 0.0183 - acc: 0.9948\n",
      "Epoch 16/20\n",
      " - 120s - loss: 0.0163 - acc: 0.9953\n",
      "Epoch 17/20\n",
      " - 120s - loss: 0.0162 - acc: 0.9950\n",
      "Epoch 18/20\n",
      " - 120s - loss: 0.0145 - acc: 0.9956\n",
      "Epoch 19/20\n",
      " - 120s - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 20/20\n",
      " - 120s - loss: 0.0116 - acc: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1432d6668>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "#print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Discussion:\n",
    "Give bullet pints summary of what you have found from above experiments.\n",
    "e.g. \n",
    "- how is a 1-layer ANN vs. 1-layer CNN?\n",
    "    * a one layer ANN performs much worse, but much faster than a convolutional neural network. In comparison would be 36% accuracy to 99.7%\n",
    "- when the number of layers increases, what happens to the performance?\n",
    "    * when the number of layers increases, the performance improves, however does leave risk for overfitting\n",
    "- what happens if I have very deep CNN? did you face with any problems?\n",
    "    * the deeper the cnn, the greater the computational power it took to run. The accuracy of a simple CNN was nearly perfect, but operated much quicker\n",
    "- what can you do to reduce an overfitting problem?\n",
    "    * to reduce overfitting we can use stride, pooling and drop out. Each of which help reduce smaller sensitities to averages/min/max/sum or cross pool relationships\n",
    "- search internet and list other ways to reduce overfitting in CNN\n",
    "    * Steps for reducing overfitting via <https://towardsdatascience.com/deep-learning-3-more-on-cnns-handling-overfitting-2bd5d99abe5d>:\n",
    "        1. Add more data\n",
    "        2. Use data augmentation\n",
    "        3. Use architectures that generalize well\n",
    "            * pooling\n",
    "            * stride\n",
    "            * not too specific of depth\n",
    "            * normalization\n",
    "        4. Add regularization (mostly dropout, L1/L2 regularization are also possible)\n",
    "        5. Reduce architecture complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
