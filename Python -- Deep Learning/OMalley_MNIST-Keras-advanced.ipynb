{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST- handwritten digit recognition Part 2\n",
    "In this modeule, we will talk about how we can further improve performance using various techniques.\n",
    "\n",
    "## Batch Normalization\n",
    "Do you remember we normalized input images such that they have zero mean? The training converges faster when images are normalized (zero mean and unit variance) and decorrelated. However, the parameter update during the training changes distributions in each layer, which is called *internal covariant shift*. Ioffe and Szegedy suggested [batch normalization](https://arxiv.org/abs/1502.03167) to normalize and decorrelate inputs to the mid-layers to resolve this issue and make the netwrok training converges faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Batch Normalization\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "# backend\n",
    "K.set_image_dim_ordering( 'tf' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 123\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][width][height][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create unit udf to be created 4 times by a loop in the BN_model\n",
    "\n",
    "* Include batch normalization in the input and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BN_model(): \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(32,(3, 3)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(64,(3, 3)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 500s - loss: 0.2078 - acc: 0.9441 - val_loss: 0.1336 - val_acc: 0.9578\n",
      "Epoch 2/20\n",
      " - 584s - loss: 0.0442 - acc: 0.9867 - val_loss: 0.0494 - val_acc: 0.9844\n",
      "Epoch 3/20\n",
      " - 499s - loss: 0.0287 - acc: 0.9918 - val_loss: 0.0456 - val_acc: 0.9849\n",
      "Epoch 4/20\n",
      " - 473s - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0350 - val_acc: 0.9896\n",
      "Epoch 5/20\n",
      " - 483s - loss: 0.0170 - acc: 0.9949 - val_loss: 0.0311 - val_acc: 0.9907\n",
      "Epoch 6/20\n",
      " - 565s - loss: 0.0147 - acc: 0.9955 - val_loss: 0.0319 - val_acc: 0.9904\n",
      "Epoch 7/20\n",
      " - 569s - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0276 - val_acc: 0.9924\n",
      "Epoch 8/20\n",
      " - 534s - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0294 - val_acc: 0.9904\n",
      "Epoch 9/20\n",
      " - 463s - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0335 - val_acc: 0.9892\n",
      "Epoch 10/20\n",
      " - 460s - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0274 - val_acc: 0.9911\n",
      "Epoch 11/20\n",
      " - 458s - loss: 0.0072 - acc: 0.9977 - val_loss: 0.0536 - val_acc: 0.9859\n",
      "Epoch 12/20\n",
      " - 456s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0274 - val_acc: 0.9925\n",
      "Epoch 13/20\n",
      " - 456s - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0281 - val_acc: 0.9924\n",
      "Epoch 14/20\n",
      " - 457s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0313 - val_acc: 0.9906\n",
      "Epoch 15/20\n",
      " - 456s - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0360 - val_acc: 0.9913\n",
      "Epoch 16/20\n",
      " - 457s - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0635 - val_acc: 0.9835\n",
      "Epoch 17/20\n",
      " - 457s - loss: 0.0073 - acc: 0.9973 - val_loss: 0.0345 - val_acc: 0.9907\n",
      "Epoch 18/20\n",
      " - 463s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0469 - val_acc: 0.9877\n",
      "Epoch 19/20\n",
      " - 458s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0266 - val_acc: 0.9935\n",
      "Epoch 20/20\n",
      " - 456s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0522 - val_acc: 0.9862\n",
      "CNN Error with proper batch implemention: 1.38%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = BN_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error with proper batch implemention: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, can you get test error below 0.5%?\n",
    "\n",
    "On the model above I was able to get the loss down to 2 basis points of a percent, but the error on the test data was 1.38, suggesting some overfitting occurred. Below, with the incorrect implementation, I was able to get the model to perform with only 0.92% error, which is still not as good as we need, but better than the model with correctly implemented batch normalization. I think a solution here could be a higher dropoff rate with the model above.\n",
    "\n",
    "Where should you position the batch norm layer to implement the batch norm correctly?\n",
    "\n",
    "From everything I've read and researched, you place the batch normalization before after the colvolution but before the activation step of the CNN process.\n",
    "\n",
    "**Claim:** Some people argue that they can get as good or better result by incorrectly implementing batchnorm such that the batchnorm comes after the activation layer. Test if this is true. What test error do you get?\n",
    "\n",
    "I end up getting a CNN error of 0.92% when I implement batch normalization after the activation function, which is actually an improvement from the proper implementation, however I believe the proper implementation was overfit and could be tuned to be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Batch Normalization - after the activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit(model, n_filter=16, init = False, dropout=True):\n",
    "    if init:\n",
    "        model.add(Conv2D(16,3, input_shape = (28,28,1), activation = 'relu', padding = 'same'))\n",
    "        model.add(Conv2D(12,3, activation = 'relu', padding = 'same'))\n",
    "        model.add(BatchNormalization())\n",
    "    else:\n",
    "        model.add(Conv2D(12,3, activation = 'relu', padding = 'same'))\n",
    "        model.add(Conv2D(12,3, activation = 'relu', padding = 'same'))\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    \n",
    "    if dropout:\n",
    "        model.add(Dropout(0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BNr_model(n, dropout=True):\n",
    "### YOUR TURN\n",
    "    # Create a model with 4 convolutional layers (2 repeating VGG stype units) and 2 dense layers before the output\n",
    "    # Use Batch Normalization for every conv and dense layers\n",
    "    model = Sequential()\n",
    "    model = unit(model, init=True, dropout = dropout)\n",
    "    if n > 1:\n",
    "        for i in range(1,n):\n",
    "            filters = min(16*2**i,512)\n",
    "            model = unit(model, n_filter = filters)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/Python36/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 129s - loss: 1.0869 - acc: 0.6324 - val_loss: 0.1890 - val_acc: 0.9469\n",
      "Epoch 2/20\n",
      " - 127s - loss: 0.2925 - acc: 0.9144 - val_loss: 0.0960 - val_acc: 0.9700\n",
      "Epoch 3/20\n",
      " - 127s - loss: 0.1948 - acc: 0.9451 - val_loss: 0.0763 - val_acc: 0.9780\n",
      "Epoch 4/20\n",
      " - 127s - loss: 0.1557 - acc: 0.9566 - val_loss: 0.0648 - val_acc: 0.9820\n",
      "Epoch 5/20\n",
      " - 127s - loss: 0.1333 - acc: 0.9631 - val_loss: 0.0627 - val_acc: 0.9805\n",
      "Epoch 6/20\n",
      " - 127s - loss: 0.1203 - acc: 0.9667 - val_loss: 0.0532 - val_acc: 0.9841\n",
      "Epoch 7/20\n",
      " - 127s - loss: 0.1076 - acc: 0.9697 - val_loss: 0.0571 - val_acc: 0.9826\n",
      "Epoch 8/20\n",
      " - 127s - loss: 0.1002 - acc: 0.9715 - val_loss: 0.0465 - val_acc: 0.9857\n",
      "Epoch 9/20\n",
      " - 127s - loss: 0.0915 - acc: 0.9742 - val_loss: 0.0380 - val_acc: 0.9874\n",
      "Epoch 10/20\n",
      " - 127s - loss: 0.0868 - acc: 0.9750 - val_loss: 0.0387 - val_acc: 0.9885\n",
      "Epoch 11/20\n",
      " - 143s - loss: 0.0813 - acc: 0.9771 - val_loss: 0.0449 - val_acc: 0.9859\n",
      "Epoch 12/20\n",
      " - 135s - loss: 0.0799 - acc: 0.9775 - val_loss: 0.0499 - val_acc: 0.9850\n",
      "Epoch 13/20\n",
      " - 128s - loss: 0.0768 - acc: 0.9785 - val_loss: 0.0411 - val_acc: 0.9874\n",
      "Epoch 14/20\n",
      " - 128s - loss: 0.0721 - acc: 0.9793 - val_loss: 0.0284 - val_acc: 0.9915\n",
      "Epoch 15/20\n",
      " - 129s - loss: 0.0671 - acc: 0.9806 - val_loss: 0.0389 - val_acc: 0.9883\n",
      "Epoch 16/20\n",
      " - 128s - loss: 0.0724 - acc: 0.9791 - val_loss: 0.0325 - val_acc: 0.9891\n",
      "Epoch 17/20\n",
      " - 128s - loss: 0.0645 - acc: 0.9814 - val_loss: 0.0350 - val_acc: 0.9891\n",
      "Epoch 18/20\n",
      " - 128s - loss: 0.0632 - acc: 0.9821 - val_loss: 0.0323 - val_acc: 0.9914\n",
      "Epoch 19/20\n",
      " - 128s - loss: 0.0635 - acc: 0.9814 - val_loss: 0.0319 - val_acc: 0.9903\n",
      "Epoch 20/20\n",
      " - 128s - loss: 0.0597 - acc: 0.9834 - val_loss: 0.0294 - val_acc: 0.9908\n",
      "CNN Error with improper batch implemention: 0.92%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = BNr_model(4)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error with improper batch implemention: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording loss and metric\n",
    "The output of `model.fit` by default (in Keras 2) returns a dictionary of model history (also it can be called using the callback). The dictionary has keys loss and metric (when you specified the metric in the model.complie) for train and validation each. For our case here it would be: 'val_loss', 'val_acc', 'loss', 'acc'. A good use of such log is to monitor whether it's over fitting. When overfits, you will see the validation loss may go up at some point while train loss continues go down. Let's get rid of batch norm layers and run the model with higher running rate lr=0.01 and longer epoch (50) to see if it overfits (Answer: Yes it does, quite terribly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def model_overfit():\n",
    "### YOUR TURN\n",
    "    # 1) Create a model with the same architecture above (4 convs and 2 denses before output) and hyperparameters, \n",
    "    # but without any batch normalization and dropouts.\n",
    "    # 2) To make this overfit surely, let's change the learning rate of our Adam optimizer. Set the learning rate to 0.01.\n",
    "    # 3) After running the training, plot the train and validation accuracy using the model output hisoty.\n",
    "    \n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = model_overfit()\n",
    "\n",
    "# Fit the model\n",
    "t0=time.time()\n",
    "log = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=200, verbose=2)\n",
    "t1=time.time()\n",
    "print(t1-t0,\" seconds\")\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Learning rate\n",
    "Without inserting batchnorm or dropout again, decrease learning rate and run for 50 epochs, plot the accuracy from train and validation. What is the highest learning rate that it doesn't overfit? What is the validation accuracy as a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Dropout\n",
    "Now, add dropouts and run with the same hyperparameters (learning rate, epochs) you found from above. Time the model.fit() using `time.time`. \n",
    "1) Does it take longer training time by adding dropouts?\n",
    "2) For the same epoch, is your final validation accuracy better? If not better and you're sure it's not overfitting yet, try to increase either your learning rate or epoch, OR change your dropout rate(s). Record your optimum values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Batch Normalization\n",
    "Now, get rid of dropouts and add batch normalization layers. Choose learning rate between 0.01 and 0.001. Find the largest learning rate that still does not overfit but gives highest accuracy.\n",
    "Time model.fit() using `time.time`. \n",
    "Plot the 'acc' and 'val_acc'\n",
    "Compare the learning rate with those from Exercise 1 and 2. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz.\n",
    "\n",
    "#### 1. \n",
    "What are the advantages of a CNN over a fully connected ANN for image classificaion?\n",
    "\n",
    "#### 2. \n",
    "Consider a CNN composed of 3 convolutional layers, each with 3x3 kernels, a stride of 2, and with 'same' padding. The first layer outputs a featuremap with 100 cahnnels, the second layer outputs a featuremap with 200 depth, and the last outputs one with 400 depth. The input is color (RGB) images of 200x300 pixels. What is the total number of parameters for this CNN model?\n",
    "\n",
    "#### 3.\n",
    "If your GPU runs out of memory while you train a CNN model, what can you do resolving the issue? List at least 3 ways to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
